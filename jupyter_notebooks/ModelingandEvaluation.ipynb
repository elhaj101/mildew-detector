{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation\n",
    "\n",
    "This notebook handles data splitting, augmentation, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Set random seeds\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "Splitting the dataset into Train, Validation, and Test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "source_dir = '../data/cherry-leaves'\n",
    "split_dir = '../data/split'\n",
    "labels = ['healthy', 'powdery_mildew']\n",
    "split_ratios = (0.7, 0.15, 0.15) # Train, Val, Test\n",
    "\n",
    "# Create split directories\n",
    "for label in labels:\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(split_dir, split, label), exist_ok=True)\n",
    "\n",
    "# Distribute images\n",
    "for label in labels:\n",
    "    src_label_dir = os.path.join(source_dir, label)\n",
    "    images = [f for f in os.listdir(src_label_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    random.shuffle(images)\n",
    "    \n",
    "    train_count = int(len(images) * split_ratios[0])\n",
    "    val_count = int(len(images) * split_ratios[1])\n",
    "    \n",
    "    train_imgs = images[:train_count]\n",
    "    val_imgs = images[train_count:train_count+val_count]\n",
    "    test_imgs = images[train_count+val_count:]\n",
    "    \n",
    "    # Helper to copy\n",
    "    def copy_images(img_list, split_name):\n",
    "        dest = os.path.join(split_dir, split_name, label)\n",
    "        # Check if already populated to avoid re-copying overhead if run multiple times\n",
    "        # But for robustness, we might just overwrite or skip.\n",
    "        # Let's check counts.\n",
    "        if len(os.listdir(dest)) == len(img_list):\n",
    "            print(f\"Split {split_name}/{label} already exists.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Copying {len(img_list)} images to {split_name}/{label}...\")\n",
    "        for img in img_list:\n",
    "            src = os.path.join(src_label_dir, img)\n",
    "            dst = os.path.join(dest, img)\n",
    "            if not os.path.exists(dst):\n",
    "                shutil.copy(src, dst)\n",
    "\n",
    "    copy_images(train_imgs, 'train')\n",
    "    copy_images(val_imgs, 'val')\n",
    "    copy_images(test_imgs, 'test')\n",
    "    \n",
    "print(\"Data splitting complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators with Augmentation\n",
    "\n",
    "We apply data augmentation (rotation, zoom, flips) to the training set to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_size = (100, 100) # Using 100x100 as per common practice for this dataset, or user previous 224? \n",
    "# Previous code used 224. But typical for this project is often smaller if not using transfer learning. \n",
    "# Let's stick to 100x100 to be safe on memory/speed, or 224 if we want higher res. \n",
    "# README doesn't specify size. Visualization nb used (100, 100).\n",
    "# I will use (100,100) to match Visualization nb.\n",
    "img_shape = (100, 100, 3)\n",
    "batch_size = 20\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_path = os.path.join(split_dir, 'train')\n",
    "val_path = os.path.join(split_dir, 'val')\n",
    "test_path = os.path.join(split_dir, 'test')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = test_datagen.flow_from_directory(\n",
    "    val_path,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_path,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "CNN Model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Input(shape=img_shape),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = '../out/modeling'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(output_dir, 'mildew_detector_model.keras')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=max(1, len(train_generator)), \n",
    "    validation_data=val_generator,\n",
    "    validation_steps=max(1, len(val_generator)),\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "\n",
    "Plotting accuracy and loss over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.title('Model Training History')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss, acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Test Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Predictions\n",
    "pred_probs = model.predict(test_generator)\n",
    "pred_classes = (pred_probs > 0.5).astype(int).flatten()\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_classes, pred_classes)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Healthy', 'Powdery Mildew'], \n",
    "            yticklabels=['Healthy', 'Powdery Mildew'])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(true_classes, pred_classes, target_names=['Healthy', 'Powdery Mildew']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}